{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4장 : 머신러닝의 기본요소\n",
    "---\n",
    "\n",
    "### 구성\n",
    "1. 머신러닝의 네가지 분류\n",
    "2. 머신 러닝 모델 평가\n",
    "3. 데이터 전처리, 특성공학, 특성 학습\n",
    "4. 과대 적합과 과소 적합\n",
    "5. 보편적인 머신러닝 작업 흐름\n",
    "6. 요약\n",
    "\n",
    "\n",
    "### 이 장에서 다룰 핵심 내용\n",
    "* 분류와 회귀 이외의 머신 러닝 형태\n",
    "* 머신 러닝 모델의 올바른 평가 과정\n",
    "* 딥러닝을 위한 데이터 전처리\n",
    "* 특성 공학\n",
    "* 과대적합 문제 해결\n",
    "* 머신 러닝 문제를 다루는 일반적인 작업 흐름"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 머신러닝 모델 평가\n",
    "훈련에서 사용된 동일ㅇ한 데이터로 모델을 평가하지 않은 이유는 **몇번의 에포크 이후로 과대적합되기 시작했기 때문**입니다. 머신러닝의 목표는 처음 본 데이터에서 잘 작동하는 일반화된 모델을 얻는 것 입니다. 여기에서 과대적합은 주요 장애물입니다. 관측할 수 있는 것만 제어할 수 있으므로 모델의 일반화 성능에 대한 신뢰할 수 있는 측정방법이 아주 중요합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 훈련, 검증, 테스트 세트\n",
    "모델 평가의 핵심은 *가용한 데이터를 항상 훈련, 검증 테스트 3개의 세트로 나누는 것*입니다. 훈련 세트에서 모델을 훈련하고 검증 세트에서 모델을 평가합니다. 모델을 출시할 준비가 되면 테스트세트에서 최종적으로 딱 한 번 모델을 테스트합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 훈련 셋, 테스트 셋 총 2개만 사용하면 어떤가?\n",
    "    * 훨씬 간단한 것 같지만, 이렇게 하지 않는 이유는 **모델을 개발할 때 항상 모델의 설정을 튜닝**하기 때문입니다. (Hyperparameter ; 층의 수나 유닛 층의 수\n",
    "    * 검증 세트에서 모델의 성능을 평가하여 이런 튜닝을 수행합니다. 본질적으로 이런 튜닝도 어떤 파라미터 공간에서 좋은 설정을 찾는 **학습** 입니다.\n",
    "    * 검증 세트의 성능을 기반으로 모델의 설정을 튜닝하면 검증 세트로 모델을 직접 훈련하지 않더라도 빠르게 검증 세트에 과대적합될 수 있습니다.\n",
    "    \n",
    "\n",
    "* 정보 누설 (Information Leak) : 검증 세트의 모델 성능에 기반하여 모델의 하이퍼파라미터를 조정할 때마다 검증 데이터에 관한 정보가 모델로 새는 것입니다. 검증 데이터 세트에 맞춰 최적화했기 때문에 검증 데이터에 의도적으로 잘 수행되는 모델이 만들어집니다. 검증데이터가 아니고 완전히 새로운 데이터에 대한 성능이 관심 대상이라면 모델을 평가하기 위해 이전에 본 적 없는 완전히 다른 데이터셋을 사용해야합니다. 이것이 바로 *테스트 데이터*입니다.\n",
    "\n",
    "\n",
    "* 데이터를 훈련, 검증, 테스트 세트로 나누는 것은 간단해보일 수 있지만 몇가지 고급 기법을 사용하면 도움이 됩니다. : hold-out validation, K-fold cross-validation, Shuffling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 단순 홀드아웃 검증 (Hold-out Variation)\n",
    "데이터의 일정량을 테스트 세트로 떄어놓는다. 정보 누설을 막기 위해 테스트 세트를 사용하여 모델 튜닝을 하면 안됩니다. 이런 이유로 검증 세트도 따로 떼어 놓아야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "num_validation_samples = 10000\n",
    "\n",
    "np.random.shuffle(data)\n",
    "\n",
    "# 검증 세트를 만듭니다.\n",
    "validation_data = data[:num_validation_samples]\n",
    "data = data[num_validation_samples:]\n",
    "\n",
    "# 훈련 세트를 만듭니다.\n",
    "training_data = data[:]\n",
    "\n",
    "# 훈련 세트에서 모델을 훈련하고 검증세트로 평가합니다.\n",
    "model = get_model()\n",
    "model.train(training_data)\n",
    "validation_score = model.evaluate(validation_data)\n",
    "\n",
    "### 여기서 모델을 튜닝하고\n",
    "### 다시 훈련 -> 평가 -> 튜닝 의 반복\n",
    "\n",
    "# 하이퍼파라미터 튜닝이 끝나면 테스트 데이터를 제외한 모든 데이터를 사용하여 모델을 다시 훈련시킵니다.\n",
    "model = get_model()\n",
    "model.train(np.concatenate([training_data,\n",
    "                           validation_data]))\n",
    "test_score = model.evaluate(testdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 평가 방법은 단순해서 한가지 단점이 있습니다. 데이터가 적을 때는 검증 세트와 테스트의 샘플이 너무 적어 주어진 전체 데이터를 통계적으로 대표하지 못할 수도 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-겹 교차 검증 (K-Fold Cross-validation)\n",
    "\n",
    "\n",
    "데이터를 동일한 크기를 가진 K개 분할로 나눕니다. 각 분할 i 에 대하여 남은 K - 1개의 분할로 모델을 훈련하고 분할 i에서 모델을 평가합니다. 최종 점수는 **이렇게 얻은 K개의 점수를 평균합니다.** 모델의 성능이 데이터 분할에 따라 편차가 클 때 도움이 됩니다. 단순 홀드아웃 검증과 똑같이 모델의 튜닝에 별개의 검증세트를 사용하게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 4\n",
    "num_validation_samples =len(data) / 4\n",
    "\n",
    "np.random.shuffle(data)\n",
    "\n",
    "validation_scores = []\n",
    "for fold in range(k):\n",
    "    validation_data = data[num_validation_samples * fold:num_validation_samples * (fold + 1)]\n",
    "    training_data = data[:num_validation_samples * fold] + data[num_validation_samples * (fold+1):]\n",
    "    \n",
    "model = get_model()\n",
    "model.train(training_data)\n",
    "validation_score = model.evaluate(validation_data)\n",
    "validation_scores.append(validation_score)\n",
    "\n",
    "validation_score  =  np.average(validation_score)\n",
    "\n",
    "model = get_model()\n",
    "model.train(data)\n",
    "test_score = model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
